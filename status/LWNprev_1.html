<!DOCTYPE html>
<html lang="en">
<head><base href="https://rss-bridge.org/bridge01/" target="_blank">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/ >
    <meta name="description" content="RSS-Bridge" />
    <title>LWN Free Weekly Edition</title>
    <link href="static/style.css?2023-03-24" rel="stylesheet">
    <link rel="icon" type="image/png" href="static/favicon.png">

    
        <link
            href="?action=display&amp;bridge=LWNprevBridge&amp;format=Atom"
            title="Atom"
            rel="alternate"
            type="application/atom+xml"
        >
	
        <link
            href="?action=display&amp;bridge=LWNprevBridge&amp;format=Json"
            title="Json"
            rel="alternate"
            type="application/json"
        >
	
        <link
            href="?action=display&amp;bridge=LWNprevBridge&amp;format=Mrss"
            title="Mrss"
            rel="alternate"
            type="application/rss+xml"
        >
	
        <link
            href="?action=display&amp;bridge=LWNprevBridge&amp;format=Plaintext"
            title="Plaintext"
            rel="alternate"
            type="text/plain"
        >
	
        <link
            href="?action=display&amp;bridge=LWNprevBridge&amp;format=Sfeed"
            title="Sfeed"
            rel="alternate"
            type="text/plain"
        >
	
    <meta name="robots" content="noindex, follow">
</head>

<body>
    <div class="container">

        <h1 class="pagetitle">
            <a href="https://lwn.net/free/bigpage" target="_blank">LWN Free Weekly Edition</a>
        </h1>

        <div class="buttons">
            <a href="./#bridge-LWNprevBridge">
                <button class="backbutton">← back to rss-bridge</button>
            </a>

                            <a href="?action=display&amp;bridge=LWNprevBridge&amp;format=Atom">
                    <button class="rss-feed">
                        Atom                    </button>
                </a>
                            <a href="?action=display&amp;bridge=LWNprevBridge&amp;format=Json">
                    <button class="rss-feed">
                        Json                    </button>
                </a>
                            <a href="?action=display&amp;bridge=LWNprevBridge&amp;format=Mrss">
                    <button class="rss-feed">
                        Mrss                    </button>
                </a>
                            <a href="?action=display&amp;bridge=LWNprevBridge&amp;format=Plaintext">
                    <button class="rss-feed">
                        Plaintext                    </button>
                </a>
                            <a href="?action=display&amp;bridge=LWNprevBridge&amp;format=Sfeed">
                    <button class="rss-feed">
                        Sfeed                    </button>
                </a>
            
                    </div>

                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="https://lwn.net//Articles/992992/"
                    >In search of the AOSP community</a>
                </h2>

                                    <time datetime="2024-10-10 00:00:00">
                        2024-10-10 00:00:00                    </time>
                    <p></p>
                
                                    <p class="author">by: Jonathan Corbet</p>
                
                <!-- Intentionally not escaping for html context -->
                

<div class="FeatureByline">
           By <b>Jonathan Corbet</b><br></br>October 7, 2024
           <hr></hr>
<a href="/Archives/ConferenceIndex/#Linux_Plumbers_Conference-2024">LPC</a>
</div>
The core of the Android operating system, as represented by the <a href="https://source.android.com/">Android Open Source Project</a> (AOSP),
can only be considered one of the most successful open-source initiatives
ever created; its user count is measured in the billions.  But few would
consider it to be a truly community-oriented project.  At the 2024 <a href="https://lpc.events/">Linux Plumbers Conference</a>, Chris Simmonds
asked why the AOSP community is so hard to find, and what might be done
about the situation.
<p>
There are, he said, many developers working on AOSP — somewhere between
10,000 and 100,000 of them, many of whom are focused on getting Android
working on a specific platform.  This is not a small group of people, but
they are nearly invisible.  The situation is, he said, a sort of <a href="https://en.wikipedia.org/wiki/Fermi_paradox">Fermi paradox</a> — they
have to be out there, but we cannot see them.  He suggested comparing AOSP
to <a href="https://www.yoctoproject.org/">the Yocto Project</a>, which
has some similar goals.  Unlike AOSP, Yocto has a vibrant and highly
visible community.
</p><p>
AOSP developers tend to work in isolation; there
is little communication between teams working on AOSP, even when they are
housed in the same company.  AOSP developers have few opportunities to get
together, share a beer, and talk about their work — or even to get together
online.  So these developers do not feel like part of a larger community
working on common problems.
</p><p>
A stronger community around AOSP would be beneficial to all involved, he said.
It would be no threat to device vendors, who would surely retain control
over their products and need not fear the loss of their intellectual
property.  Sharing work and experience would benefit both developers and
companies, just like it does with other open-source projects.  This sharing
would pay off in terms of usable code, but also in recognition for work
done and influence over the direction of the project as a whole.
Maintenance work could be shared rather than duplicated.  These benefits,
Simmonds pointed out, would accrue to Google as well as to any other
participant.
</p><p>
Yocto is in many ways similar to AOSP; it is used to build Linux-based
systems for embedded applications.  Its community is not huge, but it is
effective.  Yocto has mailing lists with people who will actually respond
to questions.  There are people who can receive and apply patches.  Yocto
has conferences where developers can meet and an open decision-making
process.  All of these resources help Yocto developers to get their work
done and to feel like part of something bigger than their own projects.
</p><p>
What does AOSP have?  Not a whole lot, he said.  There are a few
"<q>useless</q>" Google groups where nobody answers emails.  There is a
certain amount of material — some of which is better than the rest — on
Stack Overflow.  An occasional meetup is scheduled, and developers can meet
each other at certain conferences (such as the Linux Plumbers Conference).
There are forums for specific communities, and groups that have formed
around targets like <a href="https://lineageos.org/">LineageOS</a> or <a href="https://grapheneos.org/">GrapheneOS</a>.  It is not nothing, but it
is not the sort of community that a project with the impact of AOSP should
have.
</p><p>
So what can be done?  One possible answer is "nothing", but Simmonds said
that is not really an option.  There are some relatively easy steps that
would improve the situation, starting with the creation of some better
communication channels — Matrix and/or IRC channels, for example — for AOSP
developers.  Channels on social-media platforms (Mastodon and such) would
be good to have.  A common site to share code and information would be
helpful.  Creating ways for AOSP developers to meet would foster community;
online meetings would be a start, but in-person meetings are also
important.
</p><p>
Many of these things are happening in some form now, he said, but the
efforts are fragmented and lack the necessary reach.  Focusing on a
specific set of activities and resources would help this community to grow.
Also important, he said, is for all AOSP developers to become advocates for
an independent AOSP community, and to push Google to take this community
seriously.
</p><p>
Those are the easy jobs.  A harder task would be to find people to
volunteer for leadership roles within the AOSP community.  Eventually, a
separate, independent organization, with its own funding, will be needed
to support a strong and growing AOSP community.
Simmonds closed his brief talk with a request for suggestions on ways to
push this vision forward.
</p><p>
Bernhard Rosenkränzer said that one of the biggest problems is the
gatekeeping role that Google plays with AOSP.  No patches that the company
dislikes get into AOSP, regardless of what a wider community might think of
them.  A more open AOSP repository with more community-oriented acceptance
practices would help there.  Simmonds responded that this kind of change is
unlikely to happen.  A separate repository could indeed be created, but it
would have to track Google's AOSP repository; it could have additional
code, but could not be a hard fork, he said.
</p><p>
Another audience member came to Google's defense, saying that the company
has been helping in a number of ways.  Google has been investing in
developers and working to provide good tools.  What is really needed, he
said, is just for AOSP developers to talk to each other more.  Simmonds
answered that he certainly does not want to be antagonistic toward Google;
the company has released code that is used by billions of people, after all.
Few other companies have done anything like that.  But, he said, there is a
piece missing that could be filled in.
</p><p>
Bradley Kuhn pointed out that many of the groups working with AOSP code do
not operate like traditional communities; he was speaking of projects like
LineageOS.  They discuss their work on XDA forums and similar sites.
LineageOS developers, he said, do not attend the Linux Plumbers Conference.
Figuring out how to reach those people and knit them into a wider community
would be a difficult (but important) task.
</p><p>
As the session closed, Simmonds asked for anybody who is interested in
helping with this effort to contact him directly.  His hope is to put
together some sort of proposal that can be acted on in the near future.
</p><p>
The <a href="https://lpc.events/event/18/contributions/1707/attachments/1538/3217/android-mc-community.pdf">slides
from this talk</a> are available.
</p><p>
[ Thanks to the Linux Foundation, LWN's travel sponsor, for supporting our
travel to LPC. ]
</p><p><a href="/Articles/992992/#Comments">Comments (36 posted)</a>
</p><p>
<a name="993073"></a></p>
                
                            </section>
                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="https://lwn.net//Articles/993073/"
                    >The Open Source Pledge: peer pressure to pay maintainers</a>
                </h2>

                                    <time datetime="2024-10-10 00:00:00">
                        2024-10-10 00:00:00                    </time>
                    <p></p>
                
                                    <p class="author">by: Joe Brockmeier</p>
                
                <!-- Intentionally not escaping for html context -->
                

<div class="FeatureByline">
           By <b>Joe Brockmeier</b><br></br>October 8, 2024
           </div>
<p>In the early days of open source, it was a struggle to get companies
to accept the concept and trust its development model.
Now, companies have few qualms about using it, but do tend to <a href="https://www.goodtechthings.com/oss-sos/">take open source and
those who maintain it for granted</a>. The struggle now is to find ways
to compensate producers of the software, sustain the open‑source
commons, and avoid burning out maintainers. The <a href="https://opensourcepledge.com/">Open Source Pledge</a> project is
an effort to persuade companies to pay maintainers by making it a social
norm. On October 8, the project is launching a marketing campaign to raise
awareness and try to get a larger conversation started around paying
maintainers.</p>

<p>Within the open-source community itself, there is no lack of
awareness: people have been raising the alarm for years that open‑source
projects are under‑funded, that maintainers are overworked and in
danger of burnout. The xkcd <a href="https://xkcd.com/2347/">dependency
comic</a> has appeared in so many presentations, reports, and blog posts
about the topic that "random person in Nebraska" is part of the lexicon.
And just in case anyone had missed the memo, the <a href="https://lwn.net/Articles/967866/">XZ utils backdoor</a> put a
powerful spotlight on the precarious nature of an ecosystem built on
volunteer labor and the dangers of maintainer burnout.</p>

<p>As the saying goes, admitting that there is a problem is the first
step toward recovery. Most agree that maintainers deserve to
be paid for their work, and that things would be better if the
maintainers were able to make a living working on open source.
Unfortunately, there's little consensus about who should pay and
how.</p>

<h4>How it started</h4>

<p>Planning for the project kicked off in April 2024, with inspiration
from projects like <a href="https://pledge1percent.org/">Pledge 1%</a>
and <a href="https://www.theclimatepledge.com/">The Climate
Pledge</a>. The project soft-launched in June at <a href="https://web.archive.org/web/20240613054700/https://osspledge.com/">OSSPledge.com</a>
with <a href="https://sentry.io/welcome">Sentry</a> as its
sponsor and its head of open source, Chad Whitacre, leading the
effort. The project later moved to <a href="https://github.com/opensourcepledge/opensourcepledge.com/issues/129">reduce
the focus on Sentry's sponsorship</a> and make it a more vendor-neutral project.</p>

<p>Whitacre has been trying to find solutions to maintainer funding for well more
than a decade now. He founded the (<a href="https://gratipay.news/the-end-cbfba8f50981">now-defunct</a>)
maintainer-payment platform <a href="https://gratipay.com/">Gratipay</a>
in 2012, and in 2016 he co-founded a group called <a href="https://sustainoss.org/">Sustain</a> that puts on events and hosts
working groups around making open source sustainable. Now he, and other
Open Source Pledge contributors, think that the answer, or at least
<em>an</em> answer, is to establish a new norm where companies
voluntarily pay maintainers, based roughly on their usage,
and then talk about it. The hope is that social pressure will encourage
more and more companies to participate.</p>

<h4>How it works</h4>

<p>To <a href="https://opensourcepledge.com/join/">join the pledge</a>,
companies would pay $2,000 per year (or more) for every "<q>full-time
equivalent developer on staff</q>" directly to maintainers or
foundations that sponsor open-source maintainers. I spoke to Whitacre on October 2. He
said that the rationale for focusing on the number of developers, rather than
a company's revenue or another metric, is that developers are the ones
whose productivity benefits the most from open‑source software. The $2,000 figure is
something that Whitacre <a href="https://gratipay.news/your-company-should-probably-pay-2000-per-person-for-open-source-9205443e209d">came
up with many years ago</a>, based on some back-of-the-napkin math (and
rounded down a bit):</p>

<blockquote class="bq">
Companies benefit from open source most directly in their own technical
employees' boosted productivity. There are <a href="https://en.wikipedia.org/wiki/Software_engineering_demographics">21M</a>
employed programmers in the world. To move 25,000 of them into full-time
open source work (at a 50% cost savings) and also account for
non-technical contributors, each company with their own technical
employees should pay the open source community $2,143 per year per
technical employee at the company.
</blockquote>

<p>So, if a company has 10 full-time developers on staff, its minimum
contribution should be $20,000 per year. That is a big increase from
$0, but a bargain compared to proprietary software licensing and subscriptions.</p>

<p>The project does not handle, nor does it want to handle, any of the money
itself. Companies pay their dues directly to
maintainers, using platforms like the <a href="https://opencollective.com/opensource">Open Source
Collective</a>, or give funds to <a href="https://fossfoundation.info/">foundations</a> that
pay maintainers. Whitacre said that the <a href="https://thephp.foundation/">PHP Foundation</a> was "<q>the
farthest along</q>" in terms of directly <a href="https://thephp.foundation/foundation/">funding developers</a>
to work on an open‑source project. He also said that the
project was not in the business of providing resources on how to take
payments or work with maintainers to fund them: "<q>Foundations and
platforms are the answer to that [...] let the foundations become
stewards</q>".</p>

<p>Once a company has paid, it should publish a blog post that details
the payments that it has made in the past year. The project points to
blog posts by <a href="https://blog.sentry.io/we-just-gave-500-000-dollars-to-open-source-maintainers/">Sentry</a> and <a href="https://astral.sh/blog/astral-oss-fund">Astral</a> as examples.
The post should include an itemized list of the amounts paid and to
which foundation and/or maintainers. Finally, the company needs to
create a JSON file with details about its commitments, submit a pull
request to the <a href="https://github.com/opensourcepledge/opensourcepledge.com/">project's
GitHub repository</a>, and supply its branding materials. Once that is
complete (and the pull-request is accepted), the company will show up as
a <a href="https://opensourcepledge.com/members/">member</a>.</p>

<h4>What counts</h4>

<p>Some might have questions about what counts as a payment to
maintainers. For example, does a payment to an outside developer working
on an open‑source library to improve it for a company's use count
toward the Open Source Pledge? Does it count to pay Red Hat or SUSE
for subscriptions to open‑source products? The project site
does not get into specifics, currently, but the topic is under
discussion in an <a href="https://github.com/opensourcepledge/opensourcepledge.com/issues/112">issue
on GitHub</a>.</p>

<p>The general consensus is that "<q>no strings attached</q>" donations
that provide little direct benefit to the donor are acceptable, while
payments for services rendered do not count toward the pledge. A
donation to, say, the <a href="https://www.openbsdfoundation.org/">OpenBSD Foundation</a> would count—even if the company
gets an indirect benefit like public recognition for its donation.
Targeted donations to a foundation or group that result in specific
benefits for a company would not count. For example, I asked
specifically about the Linux Foundation and whether a company's
membership in that foundation would count. He said that the goal was
to pay maintainers, and that while the Linux Foundation does pay some
kernel maintainers, its budget is primarily focused on marketing and
events so that would not count towards the pledge. Nor does having
open‑source developers on staff. Whitacre said that it's
valuable and good when companies hire people to work on open source,
but the goal of the pledge was to "<q>add a zero to every
[open-source] foundation's budget</q>" to allow the foundations to pay
developers.</p>

<p>During my conversation with Whitacre, he acknowledged that there were
many what-ifs and edge cases, but generally if money was exchanged with
expectations of specific work then it's essentially a company buying a
service or product and that does not count. However, he did welcome a
debate. "<q>If we can get everybody arguing about what it means [to
participate in the pledge], then we win.</q>"</p>

<h4>Launch</h4>

<p>On October 8, the project is planning a promotional campaign to raise
awareness and bring more companies into the fold. This includes,
Whitacre said, "<q>three of the most expensive billboards in the
world</q>" on the <a href="https://en.wikipedia.org/wiki/San_Francisco%E2%80%93Oakland_Bay_Bridge">Bay
Bridge</a> in San Francisco, and running an ad on the <a href="https://en.wikipedia.org/wiki/Nasdaq_MarketSite">Nasdaq
MarketSite</a> in New York City. Companies that join the pledge before
the launch will be recognized with an "innovator" badge to denote their
early involvement.</p>

<p>More than 20 companies have signed up for the pledge so far. Sentry
has the largest number of developers, 135, but most of the companies
that have signed up so far have fewer than ten developers. The
amount pledged per developer varies widely, with one company (<a href="https://opensourcepledge.com/members/frontend-masters/">Frontend
Masters</a>) paying $10,000 per developer for six developers. Whitacre
said he would be thrilled to have the first 200-developer or
1,000-developer company join the effort but knows that would take time.
He said that he had spoken to people at a few larger organizations but
knew that it would take much longer to be able to land those
companies.</p>

<h4>Can it work?</h4>

<p>The project is banking on what Whitacre calls "<q>social
validation</q>" to nudge companies into ponying up for open‑source
development. Essentially, the group is pinning its hopes on societal
pressure to persuade a critical mass of companies into participating and
paying into the open‑source commons through foundations or other
means.</p>

<p>It is unlikely to be <em>the</em> solution to paying maintainers, but
it might be a significant piece of a solution if the group gets
traction. It will be interesting to see if this approach works, and if
companies are susceptible to social pressure where they haven't
responded to other efforts to help sustain open source.</p>

<p></p>
<p><a href="/Articles/993073/#Comments">Comments (41 posted)</a>
</p><p>
<a name="992411"></a></p>
                
                            </section>
                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="https://lwn.net//Articles/992411/"
                    >Coping with complex cameras</a>
                </h2>

                                    <time datetime="2024-10-10 00:00:00">
                        2024-10-10 00:00:00                    </time>
                    <p></p>
                
                                    <p class="author">by: Jonathan Corbet</p>
                
                <!-- Intentionally not escaping for html context -->
                

<div class="FeatureByline">
           By <b>Jonathan Corbet</b><br></br>October 3, 2024
           <hr></hr>
<a href="/Archives/ConferenceIndex/#Linux_Plumbers_Conference-2024">LPC</a>
</div>
Cameras were never the simplest of devices for Linux to support; they have
a wide range of operating parameters and can generate high rates of data.
In recent years, though, they have become increasingly complex, stressing
the ability of the kernel's <a href="https://docs.kernel.org/userspace-api/media/index.html">media
subsystem</a> to manage them.  At the 2024 <a href="https://lpc.events/">Linux Plumbers Conference</a>, developers from
that subsystem and beyond gathered to discuss the state of affairs and how
complex camera devices should be supported in the future.
<p>
</p><h4>The complex-camera summit</h4>
<p>
Ricardo Ribalda led the session, starting with a summary of the closed-door
complex-camera summit that had been held the previous day.  "Complex
cameras" are the seemingly simple devices that are built into phones,
notebooks, and other mobile devices.  These devices do indeed collect image
data as expected, but as part of that task they also perform an enormous
amount of signal processing on the data returned from the sensor.  That
processing, which can include  demosaicing, noise removal, sharpening,
white-balance correction, image stabilization, autofocus control, contrast
adjustment, high-dynamic-range processing, face recognition, and more, is
performed in memory by a configurable pipeline of units collectively known as
an image signal processor (ISP).
</p><p>

<a href="/Articles/992418/"><img alt="[Ricardo Ribalda]" class="lthumb" src="https://static.lwn.net/images/conf/2024/lpc/RicardoRibalda-sm.png" title="Ricardo Ribalda"></img></a>

Much of this functionality is controlled via a feedback loop that passes
through user space.  The ISP must be provided with large amounts of data
that controls the processing that is to be done; these parameters can add up
to 500KB or so of data — for each frame that the processor handles.
Naturally, the format of the control data tends to be proprietary and is
different for each ISP.
</p><p>
Ribalda started by saying that the vendors of these devices would like to
use the kernel's existing media interface (often called Video4Linux or
V4L), but that subsystem was not designed for this type of memory-to-memory
ISP.  Multiple <tt>ioctl()</tt> calls are required for each frame, adding
a lot of overhead.  V4L does not provide fences for the management of
asynchronous operations; attempts have been made to add fences in the past, but
did not succeed.  There are no abstractions for advanced scheduling of
operations, and no support for modes where multiple buffers are sent to the
ISP and it decides which ones are important.
Multiple-camera support, needed for modern phones, is lacking in V4L, he
said.  
</p><p>
Vendors feel that the V4L API is simply too slow to evolve, so it is
failing to keep up with modern camera devices.  It is much faster, in the
short term at least, for vendors to just bash out a device-specific driver
for their hardware.  But that leads to multiple APIs for the same
functionality, creating fragmentation.
</p><p>
There has been talk of moving support for these devices over to the direct
rendering manager (DRM) subsystem, alongside graphics processors, but the
DRM developers are not camera experts.  There is also interest in creating
pass-through interfaces that let user space communicate directly with the
device (a topic that had been <a href="/Articles/990802/">extensively
discussed</a> at the Maintainers Summit a few days before), but allowing
such an API requires trusting vendors that have not, in turn, trusted the
kernel community enough to provide detailed information about how their
devices work.
</p><p>


The conclusion from the complex-camera summit, Ribalda said, was that the
V4L developers would like to see a complete list of technical features
needed to support complex camera devices.  They would then work to address
those shortcomings.  If they are unable to do so within a reasonable time,
he said, they agreed to not block the incorporation of ISP drivers into the
DRM subsystem, possibly with pass-through APIs, instead.
</p><p>
There are also non-technical challenges, Ribalda continued.  It is not just
that many aspects of these ISPs are undocumented; vendors claim that they
<i>cannot</i> be documented.  This area, it seems, is a minefield of
patents and "special sauce"; vendors do not want to reveal how their
hardware works.  But the V4L subsystem currently requires documentation for
any feature that users can control from user space.  Currently, the policy
means that undocumented features cannot be used; changing the policy,
though, would likely result in <i>all</i> features becoming undocumented.
</p><p>
A solution proposed at the summit was to describe a "canonical ISP" with
features that all of these devices are expected to have; those features
would need to be fully documented and implemented in standard ways.
Everything else provided by the ISP could be wired directly to user space
via a pass-through interface.  That would allow functionality to be made
available without requiring documentation of everything the device does.
</p><p>
Of course, there are problems with this approach, he said.  Pass-through
interfaces raise security concerns.  The kernel does not know what the
device is doing and cannot, for example, prevent it from being told to
overwrite unrelated data in the kernel.  These devices require calibration
for every combination of ISP, sensor, and lens; without full documentation,
that calibration cannot be done.  And, naturally, there will be
disagreement over what a canonical ISP should do; vendors will push to
implement the bare minimum possible.
</p><p>
An alternative is to ignore proprietary functionality entirely, and
document only the basic functionality of the device.  Vendors would then
provide an out-of-tree driver to make the device actually work as intended.
In this world, though, vendors are unlikely to bother with an upstream
driver at all.  The result will be hard for both distributors and users to
manage.
</p><p>
</p><h4>What to do?</h4>
<p>
This introduction was followed by an extensive, passionate, wandering, and
often loud discussion over the proper approach to take regarding complex
cameras.  The discussion was also long, far overflowing the allotted time.
Rather than trying to capture the whole thing, what follows is an attempt
to summarize the most significant points of view.  Apologies to all
participants whose contribution is not reflected here.
</p><p>
It was clear that the conclusions from the closed summit did not find a
consensus in the room.  Almost every aspect of them was questioned at one
point or another.  Nonetheless, they made a useful starting point for the
discussion that followed.
</p><p>
V4L maintainer Hans Verkuil asserted that, in his experience, the
device-specific functionality requested by vendors tends to not really be
only found in one device; this functionality should be generalized in the
interface, he said.  The DRM layer requires <a href="https://www.mesa3d.org/">Mesa</a> support (in user space) for new
drivers; that is the level where the API is standardized.  V4L, he said,
has lacked the equivalent of Mesa, so the kernel API <i>is</i> the standard
interface.
</p><p>
Now, though, the <a href="https://libcamera.org/">libcamera</a> library is
taking over the role of providing the "real" interface to camera devices,
which can change the situation.  Vendor-specific support can be implemented
there, hiding it from users.  So perhaps the best solution is to require
the existence of a libcamera interface for complex camera drivers.
Meanwhile, the V4L interface will still be needed to control the sensor
part of any processing chain; perhaps code could move to DRM for the ISP
part, if V4L support will be too long in coming.  Media subsystem
maintainer Mauro Carvalho Chehab agreed that libcamera makes a DRM-like
model more possible.
</p><p>
DRM subsystem maintainer Dave Airlie said that the existing architecture of
the V4L subsystem is simply not suitable for modern camera devices.  It is,
he said, in the same position as DRM was 20 years ago, when that
subsystem had to make a painful transition from programming device
registers to exchanging commands and data with user space via ring buffers.
If there is a libcamera API that can describe these devices in a general
way, he said, then a kernel driver for a specific device should not be
merged until libcamera support is present.  Then, he said, the ecosystem
will sort itself out over time.
</p><p>
He later added that he does not want camera drivers in the DRM subsystem,
which was not designed for them; he is willing to accept them if need be,
though.  He had really expected V4L to have evolved some DRM-like
capabilities by now; that is where the problem is.
</p><p>
The lesson from the DRM world, he said, is to just go ahead and build
something, and the situation will improve over time.  The vendors with
better drivers will win in the market.  He advised against writing
specific rules for the acceptance of drivers, saying that vendors would
always try to game them.  Instead, each driver should be merged after a
negotiation with the vendor, with the requirements being ratcheted up over
time.  That may mean allowing in some bad code initially, especially from
vendors that cooperate early on, but the bar can be raised as the quality
of the subsystem improves overall.
</p><p>
There was some discussion about how closely ISPs actually match the GPUs
driven by the DRM subsystem.  Sakari Ailus said that they are different;
they are a pipeline of processing blocks that is configured by user space.
There is only one real command: "process a frame".  Libcamera
developer Laurent Pinchart said that the current model for ISPs does not
involve a ring buffer; instead, user space submits a lot of configuration
data for each frame to be processed.  Both seemed to think that the DRM
approach might not work for this kind of device.
</p><p>
Daniel Stone said, though, that there are GPUs that operate with similar
programming models; they do not all have ring buffers.  He took strong
exception to the claim that, if pass-through functionality is provided,
vendors will have no incentive to upstream their drivers.  Often, he said,
it's not the vendors who do that work in the first place.  The Arm GPU
drivers were implemented by Collabora; Arm is only now beginning to help
with that work.  There were a lot of people who wanted open drivers, he
said, and were willing to pay for the work to be done.  So he agreed with
Airlie that the market would sort things out over time.  Manufacturers who
participate in the process will do better in the end.
</p><p>
Several participants disagreed with the premise that vendors need to keep
aspects of their hardware secret for patent or competition reasons.  As is
the case elsewhere in the technology industry, companies are well aware of
exactly what their competitors are doing.  Airlie added that all of these
companies copy each other's work, aided by engineers who move freely
between them.
</p><p>
There was also a fair amount of discussion on whether allowing pass-through
drivers would facilitate a complete reverse engineering of the hardware.
Pinchart said that the large number of parameters for each frame makes
reverse engineering difficult.  Stone replied that this argument implies
either that ISPs are different from any other device, or that the engineers
working with them are less capable than others; neither is true, he said.
Airlie added that the same was said about virtual-reality headsets, but
then "<q>one guy figured it out</q>" and free drivers were created.
Pinchart said that regular documentation for these devices would not
suffice to write a driver, again to general disagreement.
</p><p>
</p><h4>Goals</h4>
<p>
The developers in the room tried to at least coalesce on the goals they
were trying to reach.  The form of the desired API is not clear at this
point, though Pinchart said that it should not force a lot of
computation in the kernel.  Airlie suggested pushing all applications
toward the use of libcamera; eventually developers will prefer that over
working with proprietary stacks.  Pinchart was concerned that allowing
pass-through functionality would roll back the gains that have been made
with vendors so far.  So some features, at least, have to be part of the
standard API; the hard part, he said, is deciding which ones.
</p><p>
Pinchart said there seemed to be a rough agreement that vendors should be
required to provide a certain level of functionality to have drivers
merged, but he wondered how those requirements should be set.  Airlie
repeated that hard rules here would be gamed, and that there should be a
negotiation with each vendor.  If being friendly with one advances the
situation, he said, "<q>go for it</q>".  Pinchart worried that both vendors
and the community might object.  Airlie said that he just tells complaining
vendors to go away, but that the early cooperative vendors, to a great
extent, <i>are</i> the community.
</p><p>
Several developers said that the requirements could vary depending on the
market each device serves.  For a camera aimed at Chromebooks, open
functionality sufficient for basic video conferencing may be sufficient.
But for cameras to be used in other settings, the bar may be higher, with
requirement for more functionality provided by an in-tree driver.  Some
developers suggested a minimum image-quality requirement, but that would be
hard to enforce; properly measuring image quality requires a well-equipped
(and expensive) laboratory.
</p><p>
As this multi-hour session ran over time, Ribalda made some attempts to
distill a set of consensus conclusions, but he was not hugely successful.
Nonetheless, this discussion would appear to have made some headway.  There
are other kernel subsystems that have had to solve this problem in the
past, resulting in a lot of experience that can be drawn from.  Support for
complex camera devices in Linux seems likely to be messy and proprietary
for some time but, with luck, it will slowly improve.


</p><p>
[ Thanks to the Linux Foundation, LWN's travel sponsor, for supporting our
travel to this event. ]
</p><p><a href="/Articles/992411/#Comments">Comments (34 posted)</a>
</p><p>
<a name="992055"></a></p>
                
                            </section>
                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="https://lwn.net//Articles/992055/"
                    >Smart pointers for the kernel</a>
                </h2>

                                    <time datetime="2024-10-10 00:00:00">
                        2024-10-10 00:00:00                    </time>
                    <p></p>
                
                                    <p class="author">by: Daroc Alden</p>
                
                <!-- Intentionally not escaping for html context -->
                

<div class="FeatureByline">
           By <b>Daroc Alden</b><br></br>October 4, 2024
           <hr></hr>
<a href="https://lwn.net/Articles/990496/">Kangrejos 2024</a>
</div>
<p>
Rust has a plethora of smart-pointer types, including reference-counted
pointers, which have special support in the compiler to make them
easier to use. The Rust-for-Linux project would like to reap those same benefits
for its smart pointers, which need to be written by hand to conform to
the
<a href="/Articles/718628/">
Linux kernel
memory model</a>. Xiangfei Ding
presented at Kangrejos about the work to enable custom
smart pointers to function the same as built-in smart pointers.
</p>

<p>
Ding showed the specific "superpowers" that built-in smart pointers have in his
<a href="https://kangrejos.com/2024/SmartPointer%20and%20PinCoerceUnsized.pdf">
slides</a>:
unsizing and dynamic dispatch. Unsizing allows the programmer
to remove the length of an array behind a pointer from its type,
turning
a <tt>Ptr&lt;[T; N]&gt;</tt> (bounds-checked at compile time) into a
<tt>Ptr&lt;[T]&gt;</tt> (bounds-checked at run time). This needs special support because slices
(values of type <tt>[T]</tt>) do not have a known size at compile time; therefore the compiler
needs to store the size somewhere at run time. The compiler could store the size
in the pointed-to allocation, but that would require reallocating the array's
memory, which would be expensive. Instead, the compiler stores the size
alongside the pointer itself, as a fat pointer. On nightly Rust compilers, users
can enable an experimental feature and then have their
pointer type implement
<a href="https://doc.rust-lang.org/std/ops/trait.CoerceUnsized.html"><tt>CoerceUnsized</tt></a>
to indicate that it supports that.
</p>

<a href="/Articles/992336">
<img alt="[Xiangfei Ding]" class="lthumb" src="https://static.lwn.net/images/2024/xiangfei-ding-small.png" title="Xiangfei Ding"></img>
</a>

<p>
The second superpower is called
<a href="https://doc.rust-lang.org/std/ops/trait.DispatchFromDyn.html"><tt>DispatchFromDyn</tt></a>
and allows converting a <tt>Ptr&lt;T&gt;</tt> into a <tt>Ptr&lt;dyn
Trait&gt;</tt> when <tt>T</tt> implements <tt>Trait</tt>. This has to do with
the way that Rust implements dynamic dispatch — a value of type <tt>Ptr&lt;dyn
Trait&gt;</tt> uses a dispatch table to find the implementation of the method
being invoked at run time.
That method expects to receive a <tt>self</tt> pointer. So converting a smart
pointer to use dynamic dispatch only works when the smart pointer can be used as
a <tt>self</tt> pointer.
</p>

<p>
These features are both experimental, because the Rust project is still working
on their design. Ding explained that there is an
<a href="https://rust-lang.github.io/rfcs/3621-derive-smart-pointer.html">
RFC</a> aimed at stabilizing just enough for the Linux kernel to use, without
impeding the development of the features. The RFC would add a new macro
that makes it trivial for
a smart pointer satisfying certain requirements to implement the
necessary traits, no matter what the final forms of the traits end up looking
like. That would let the kernel start using its custom smart pointers on stable
Rust sooner rather than later.
</p>

<p>
There is one catch — implementing these features for a smart-pointer type with a
malicious or broken
<a href="https://doc.rust-lang.org/std/ops/trait.Deref.html">
<tt>Deref</tt></a> (the trait that lets a programmer dereference a value)
implementation could break the guarantees
Rust relies on to determine when objects can be moved in memory.
This is of particular importance to
<a href="https://doc.rust-lang.org/std/pin/struct.Pin.html"><tt>Pin</tt></a>,
which is a wrapper type used to mark an allocation that cannot be moved.
It's not hard to write smart-pointer types that don't cause problems,
but in keeping with Rust's
commitment to ensuring safe code cannot cause memory-safety problems, the RFC
also requires programmers to use unsafe (specifically, implementing an
<tt>unsafe</tt>
<a href="https://users.rust-lang.org/t/understanding-the-marker-traits/75625/3">
marker trait</a>) as a promise that
they've read the relevant documentation and are not going to break <tt>Pin</tt>.
With that addition, the code for a smart-pointer type would look like this:
</p>

<pre>
    // Use Ding's macro ...
    #[derive(SmartPointer)]
    // On a struct that is just a wrapper around a pointer
    #[repr(transparent)]
    struct MySmartPointer&lt;T: ?Sized&gt;(Box&lt;T&gt;);

    // Implement Deref, with whatever custom logic is needed
    impl&lt;T: ?Sized&gt; Deref for MySmartPointer&lt;T&gt; {
        type Target = T;
        fn deref(&amp;self) -&gt; &amp;T {
            ...
        }
    }

    // And then promise the compiler that the Deref implementation is okay to
    // use in conjunction with Pin:
    unsafe impl&lt;T: ?Sized&gt; PinCoerceUnsized for MySmartPointer&lt;T&gt; {}
</pre>

<p>
Andreas Hindborg asked for some clarification about why the marker trait is
needed. <tt>Deref</tt>
is supposed to be simple, Ding explained. Usually, someone writing a
smart-pointer type would have a normal pointer stored in their type; when implementing
<tt>Deref</tt>, they can just use the normal pointer. But it's technically
possible to implement something more complicated than that. In this case, you
could have a <tt>Deref</tt> implementation that actually moves data out of the
object pointed to and stores something else there. This would not normally be a problem,
except when the smart pointer is contained in a <tt>Pin</tt>, which is supposed
to prevent the value from being moved. If the <tt>Deref</tt> implementation
moves the value anyway, then that would be undefined behavior. The unsafe marker
trait is a promise to the compiler that the programmer has not done that.
</p>



<p>
The new macro is available on nightly Rust, although Ding says that it needs a
bit more testing in order to stabilize, as well as some additional documentation
which he is working on. Miguel Ojeda asked how soon the macro might be
stabilized; Ding answered that it should be quite soon. He will make a
stabilization report shortly, and then it is just a matter of checking off the
requirements.
</p>
<p><a href="/Articles/992055/#Comments">Comments (30 posted)</a>
</p><p>
<a name="992455"></a></p>
                
                            </section>
                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="https://lwn.net//Articles/992455/"
                    >Efficient Rust tracepoints</a>
                </h2>

                                    <time datetime="2024-10-10 00:00:00">
                        2024-10-10 00:00:00                    </time>
                    <p></p>
                
                                    <p class="author">by: Daroc Alden</p>
                
                <!-- Intentionally not escaping for html context -->
                

<div class="FeatureByline">
           By <b>Daroc Alden</b><br></br>October 8, 2024
           <hr></hr>
<a href="https://lwn.net/Articles/990496/">Kangrejos 2024</a>
</div>
<p>
Alice Ryhl has been working to enable
<a href="https://www.kernel.org/doc/html/latest/trace/tracepoints.html">
tracepoints</a> — which are widely used
throughout the kernel — to be seamlessly placed in Rust code as well. She spoke
about her approach at Kangrejos. Her
<a href="https://lwn.net/ml/all/20241001-tracepoint-v9-0-1ad3b7d78acb@google.com/">
patch set</a>
enables efficient use of static
tracepoints, but supporting dynamic tracepoints will take some additional effort.
</p>

<p>
Ryhl described tracepoints as a kind of logging that
records information from specific places in the kernel when they are reached.
She gave
<a href="https://elixir.bootlin.com/linux/v6.11.1/source/drivers/android/binder_trace.h#L22">
<tt>binder_ioctl()</tt></a> as an example of a trace event in
her <a href="https://kangrejos.com/2024/Tracepoints.pdf">slides</a>; that
tracepoint is triggered every time an <tt>ioctl()</tt> for Android's binderfs
filesystem occurs. A developer trying to
debug kernel problems can look at the log of tracepoints hit by a driver
to figure out what's happening.
</p>

<p>
In C, the programmer places a tracepoint with a line that looks like a normal
function call.
Most of the time, this call does nothing. When in use, a programmer can
attach an arbitrary function to it at run time
that will be called when the tracepoint is hit. Since
most tracepoints are disabled most of the time, Linux uses
<a href="https://docs.kernel.org/staging/static-keys.html#abstract">
static keys</a>
(patching the call into the code at run time) to make
this efficient.
</p>

<p>
Production-ready Rust drivers must be able to support the same standard of
debugging, and therefore be able to place tracepoints, Ryhl said. That could be
done today, by wrapping existing C tracepoints in Rust wrappers, but this loses
one of the most important benefits of tracepoints: their low overhead. Ideally,
hitting a disabled tracepoint from Rust should have the same performance cost as
C (i.e., almost none).
</p>

<p>
Her solution is a small Rust macro that creates the necessary static-key
machinery on the Rust side. Rust code uses <tt>declare_trace!()</tt> to refer to a
tracepoint defined in C; the macro creates an inline unsafe function on the Rust
side that can be used to trigger the tracepoint.
The generated function uses inline assembly to define
a place for the static-key machinery to patch in a call to the C tracepoint when
necessary.
</p>

<p>
Ryhl took this approach because it represents implementing the bare minimum in
Rust, leaving most of the tracepoint implementation in unchanged C, she said.
The static-key functionality has to be implemented on the Rust side for
performance, but this way she does not have to reimplement any of the
functionality for defining tracepoints, and can instead just link to the C code.
</p>

<p>
There is a catch, though. Static keys in C also use inline assembly to create a
target for the patched-in jump. In her first attempt, Ryhl copied the inline
assembly to use on the Rust side. This was
<a href="https://lwn.net/ml/all/20240606172320.GF8774@noisy.programming.kicks-ass.net/">
rejected</a> for introducing code
duplication, which is usually frowned upon in the kernel.
</p>

<p>
To solve that, Ryhl took the "<q>horrible</q>" approach of having a Rust source
file generated using the C preprocessor that gets included in the macro. The
original C sources have a comment to show where the shared inline assembly is
located, and the build system uses <tt>sed</tt> to extract it and put it in the
generated Rust file. This avoids any code duplication, at the cost of
complicating the build.
</p>

<p>
The attendees were a bit surprised at the presented solution. Paul McKenney gave
some background information on the reason that kernel developers care so much
about avoiding code duplication: in addition to the normal reasons of code
quality, it makes rebasing changes much easier. The kernel deals
with a lot of patches flying around, and any code that exists in two places can
easily get out of sync. Ryhl agreed, saying that there are good reasons
not to duplicate code. It made her life difficult, she joked, but she
sees why the static-key maintainer insisted.
</p>

<p>
Gary Guo said that
it is probably not a good idea to use the C preprocessor to generate Rust code.
Ryhl replied that it might be possible to generate both the C and Rust from a
common format, if that would be preferable. An alternative would be to teach
Rust to read C header files itself, but that is much more work. Some other
alternate ideas were floated around. McKenney was of the opinion that any
approach was acceptable — as long as it actually gets documented, because
otherwise all this unusual code-sharing is going to confuse future programmers.
</p>

<h4>Dynamic tracing</h4>

<p>
Richard Weinberger asked about dynamic tracepoints
(<a href="https://www.kernel.org/doc/html/latest/trace/kprobes.html">Kprobes</a>)
— which allow the user to
attach a tracepoint anywhere in the code using BPF. Does this work with Rust?
Ryhl was unfamiliar with the mechanism.
Andreas Hindborg suggested that addressing static tracepoints first, and then
looking into dynamic tracepoints later would make sense. Weinberger did think
that support for dynamic tracepoints would be needed eventually, because people
want their debug tooling to work throughout the whole kernel.
</p>

<p>
Ryhl thought that support for dynamic tracing would need to be added to the
Rust compiler, based on Hindborg's description of the kernel's
<a href="https://www.kernel.org/doc/html/latest/trace/ftrace.html">function
tracing</a> code.
Static tracepoints would still be needed, however, since they are
also used as a way for vendors to hook into the functions of a driver in some
cases. (Some Android hardware vendors rely on tracepoints to react to events in
the kernel, for example.)
Boqun Feng
agreed, saying that both kinds of tracepoint were needed for different use
cases. Hindborg pointed out that function tracing also interacts strangely
with function inlining — finding the location of the hook after inlining depends
on having BTF information available. So Rust will need
<a href="/Articles/991719/">
native BTF support</a> before
that is possible.
</p>

<p>
Hindborg was worried that having a solution which requires defining the
tracepoint in C as well will make it harder to have a pure-Rust solution in the
future. Ryhl responded that, although she has so far only tackled the
declaration of tracepoints in Rust, someone could in the future add the
definition of tracepoints as well.
</p>

<p>
Despite the discussion of future work, the attendees had no problems with Ryhl's
current design. It seems likely that static tracepoints will soon be usable with
Rust code in the kernel, which will enable
vendor integration with drivers written in Rust. Dynamic tracepoints and other
debugging features will take some more time.
</p>
<p><a href="/Articles/992455/#Comments">Comments (17 posted)</a>
</p><p>
<a name="992693"></a></p>
                
                            </section>
                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="https://lwn.net//Articles/992693/"
                    >Improving bindgen for the kernel</a>
                </h2>

                                    <time datetime="2024-10-10 00:00:00">
                        2024-10-10 00:00:00                    </time>
                    <p></p>
                
                                    <p class="author">by: Daroc Alden</p>
                
                <!-- Intentionally not escaping for html context -->
                

<div class="FeatureByline">
           By <b>Daroc Alden</b><br></br>October 9, 2024
           <hr></hr>
<a href="https://lwn.net/Articles/990496/">Kangrejos 2024</a>
</div>
<p>
<a href="https://rust-lang.github.io/rust-bindgen/">
Bindgen</a> is a widely used tool that automatically generates Rust bindings from C
headers. The
<a href="https://rust-for-linux.com/">
Rust-for-Linux</a> project uses it to create some of
the bindings between Rust code and the rest of the kernel. John Baublitz
presented at Kangrejos about the improvements that he has made to the tool in
order to make the generated bindings easier to use, including improved support
for macros, bitfields, and enums.
</p>

<p>
Baublitz noted that there has been a wishlist of features to add to
bindgen for the Rust-for-Linux
project for some time. After he ran into some of the same problems in his own
projects, he decided to tackle them. There are three main problems that he wants
to address: macro expansion, accessing bitfields via raw pointers, and
supporting better conversions for Rust enums.
</p>

<h4>Macro expansion</h4>

<p>
There is no way that bindgen can usefully support the full richness of C macros.
But there are a subset of macros that are useful to have represented in the
generated Rust code: macros that are just used as a name for a constant value.
Currently, bindgen specially recognizes simple macros and turns them into constants:
</p>

<pre>
    #define NAME 3
    // becomes
    pub const NAME: u32 = 3;
</pre>

<p>
However, it's relatively common for a macro to be defined in terms of other macros, which
requires expanding the macro to determine its value. Since
bindgen doesn't include a reimplementation of the C preprocessor, it
can't handle these more complex macros.
Baublitz gave the example
of
<a href="https://gitlab.com/cryptsetup/cryptsetup">
cryptsetup</a>, which added the
<a href="https://en.cppreference.com/w/cpp/header/cstdint">
<tt>UINT32_C</tt> macro</a> around some of its
constants and broke the generated Rust bindings.
</p>



<p>
He has come up with a way to make it work, however. With his changes, bindgen
can now capture the name of the macro, create a temporary C file with a main
function that returns the value of the macro, and then use Clang to compile it. Baublitz
described this as "<q>a bit hacky</q>", but working. For now, the new code
remains opt-in using the <tt>--clang-macro-fallback</tt> flag, for two reasons.
First of all, even small changes to generated bindings can cause problems, such
as by introducing duplicated names, so bindgen tries not to change the default
behavior. Secondly, the approach does have performance implications, since it
involves invoking Clang.
</p>

<p>
The performance impact isn't that bad, however. Baublitz measured the time taken
to evaluate the macros in a consolidated header file containing all of the constants
defined in the kernel's headers, which was 3-5 seconds. His initial prototype
was significantly worse, taking nearly 35 minutes. The majority of that time was
spent doing I/O; switching to Clang's in-memory API made that much faster,
but still too slow for practical work. His final design takes advantage of
Clang's support for precompiled headers, by compiling the headers once, and then
generating multiple C files in memory to evaluate the different constants.
</p>

<p>
There is one complication to using precompiled headers. Clang actually only
supports using one precompiled header per source file, and silently ignores any
others passed. So, Baublitz generates a synthetic header that imports
all of the others, and then pre-compiles that. Still, despite the problems, the new option was
released in
<a href="https://github.com/rust-lang/rust-bindgen/releases/tag/v0.70.0">
bindgen 0.70</a> on August 16, and is available to users. In the future, Baublitz
would like to add a Clang API that retains macro information when parsing, and
use that directly, instead of maintaining this workaround. Miguel Ojeda
confirmed that the two of them had spoken to a Clang maintainer, who had approved of that
approach. For now, however, this solution works, and makes many more constants
available between the two languages.
</p>

<h4>Bitfield access</h4>

<p>
Since C does not have Rust's lifetime tracking, programmers often need to refer to
structures shared between Rust and C using raw pointers instead of Rust's
references.
This poses a problem for bitfields.
Rust doesn't have a native
concept of bitfields, so when a C structure contains a bitfield, bindgen
generates accessor functions to access the value correctly. The generated
functions take a reference to the structure, since that is the idiomatic way to
define methods for a type in Rust. This poses a problem for structures that need
to be referred to with raw pointers.
</p>

<p>
Baublitz addressed this problem by adding an additional set of unsafe
helper functions to access bitfields using raw pointers. At the time of his
talk, the Rust-for-Linux developers had reviewed his code and agreed that it
would be helpful, but it still needed a review from the bindgen maintainer.
</p>

<p>
Luckily that maintainer, Christian Poveda Ruiz, was also in attendance, and
agreed to look at the pull request shortly. As of September 24, the new helpers
have
<a href="https://github.com/rust-lang/rust-bindgen/commit/3c09db0b5646b3d229d5c5f5e13cf2b2373ae2d9">
been merged</a>, and they should be available in the next release.
</p>

<h4>Enum conversions</h4>

<p>
The last item Baublitz discussed was improving how bindgen represents enums. The
problem in this case has to do with a mismatch between how C and Rust treat
invalid enum variants. In C, enums are essentially named constants, and it is not
undefined behavior assign a value to an enum variable that has not been
defined for the enum type. In Rust, creating an enum with an invalid
bit pattern, such as a nonexistent variant, is instant undefined behavior.
Because of that, bindgen currently translates C enums to compile-time constants.
</p>

<p>
It would be more convenient to translate them directly to Rust enums, since then
the compiler could then perform exhaustiveness checking and so on. Baublitz's
solution is to have two types: a raw type that is just an alias for the C enum's
storage type (such as <tt>unsigned integer</tt>), and another type that is a
normal Rust enum. Then bindgen can generate two sets of conversion functions:
safe functions that check that the enum is valid and could return an error, and
unsafe unconditional functions for when the programmer can guarantee that there
won't be any invalid values.
</p>

<p>
Changing the way enums are translated would be a breaking change, so Baublitz
has added a command-line flag — <tt>--rustified-enum</tt> — that lets users
select whether they want the old behavior, safe conversions, or unsafe
conversions. There were some challenges to making this code work, he added. He
needed to change how bindgen does its command-line parsing, and adapt some of
the internals to handle both translated and untranslated types.
</p>

<p>
The updated enum code is still in progress, however, because there are
some questions that Baublitz wants feedback on. In particular, he would like to
still
generate constants for enum values, to make switching between the different enum
translations as small a change as possible — but that could lead to problems
with namespacing.
Gary Guo suggested using associated constant items, but Baublitz
explained that bindgen currently doesn't do that in other cases, so it wouldn't
be consistent. Also, the constants would clash with the names of the actual variants.
</p>

<p>
Alice Ryhl had further questions about how the new enum translation interacts
with control-flow-integrity (CFI) protections. While there are many CFI
techniques, she specifically referred to type-based CFI, where the
compiler inserts checks that a call through a function pointer is only made to a
function of a compatible type. This cuts down on the amount of unintended
control flow an attacker can cause by overwriting function pointers. She was
worried specifically about the case where, using the new translation, the Rust
compiler sees a FFI function as taking a <tt>c_int</tt>, while the C side sees
it as taking an enum type. These types might have compatible storage layouts, but they
have different type names, which would generate different CFI tags.
Baublitz was unfamiliar with the details of CFI, and after a short back and
forth agreed with Ryhl's suggestion to add a wrapper type with the correct name.
</p>

<p>
Benno Lossin wanted to take the opportunity to explain why the new enum
translations would be helpful in the driver he is working on: currently, it has
a lot of manual checks that could ideally be simplified by having the tooling
do it. Poveda Ruiz clarified that he thinks Baublitz's style would be a sensible
default, but that every time the bindgen project changes the defaults, things
break and people complain. So while the new style may become an option, it will
not be the default.
</p>

<p>
In all, it seems like users of bindgen should have more options for correct,
ergonomic translation of C interfaces — but that they must be aware to
take advantage of them. Readers who use bindgen in their own projects might wish
to keep an eye out for Baublitz's changes.
</p>
<p><a href="/Articles/992693/#Comments">Comments (25 posted)</a>
</p><p>
<a name="992219"></a></p>
                
                            </section>
                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="https://lwn.net//Articles/992219/"
                    >ClassicPress: WordPress without the block editor</a>
                </h2>

                                    <time datetime="2024-10-10 00:00:00">
                        2024-10-10 00:00:00                    </time>
                    <p></p>
                
                                    <p class="author">by: Joe Brockmeier</p>
                
                <!-- Intentionally not escaping for html context -->
                

<div class="FeatureByline">
           By <b>Joe Brockmeier</b><br></br>October 7, 2024
           </div>
<p>The <a href="https://lwn.net/Articles/991906/">recent WordPress
controversy</a> is not the first time there's been tension between the
<a href="https://wordpress.org/">WordPress</a> community, the interests of <a href="https://automattic.com/">Automattic</a> as a business, and Matt
Mullenweg's leadership as WordPress's benevolent dictator for
life (BDFL). In particular, Mullenweg's focus on pushing WordPress to use a new
"editing experience" called <a href="https://wordpress.org/gutenberg/">Gutenberg</a> caused significant
friction—and led to the <a href="https://www.classicpress.net/">ClassicPress</a> fork. Users who
want to preserve the "classic" WordPress experience without straying
too far from the WordPress fold may want to look into ClassicPress.</p>

<h4>Gutenberg background</h4>

<p>In 2017, Mullenweg <a href="https://make.wordpress.org/core/2017/01/04/focus-tech-and-design-leads/">announced</a>
that one focus for the year would be "<q>to create a new page and post
building experience</q>". The goal was to make it easier to develop
sites using WordPress without resorting to custom HTML, <a href="https://wordpress.com/support/wordpress-editor/blocks/shortcode-block/">shortcodes</a>,
or so-called "<q>mystery meat</q>": features supported by WordPress that
were perhaps known to developers but not easily accessed by those
simply authoring content on a WordPress site.</p>

<p>For much of WordPress's history, it had used <a href="https://en.wikipedia.org/wiki/TinyMCE">TinyMCE</a> as its
default editor to create blog posts and site pages. This rich-text editor
was fine for creating posts and pages with simple layouts, but WordPress <a href="https://ma.tt/2017/08/we-called-it-gutenberg-for-a-reason/">was 
seeing competition</a> from companies like <a href="https://en.wikipedia.org/wiki/Wix.com">Wix</a> and <a href="https://en.wikipedia.org/wiki/Squarespace">Squarespace</a> that
offered drag-and-drop site-builder tools. However, while that competition
was a problem for Automattic—the community as a whole was
<a href="https://wptavern.com/matt-mullenweg-addresses-controversies-surrounding-gutenberg-at-wordcamp-portland-qa">not
convinced</a> that WordPress needed to go in that direction.</p>

<p>The <a href="https://developer.wordpress.org/block-editor/">new editor</a> took
on the name Gutenberg, a nod to German inventor <a href="https://en.wikipedia.org/wiki/Johannes_Gutenberg">Johannes
Gutenberg</a> who developed the movable-type printing press. The <a href="https://wordpress.org/documentation/wordpress-version/version-5-0/">WordPress 5.0</a>
release in 2018 included Gutenberg as part of its core features
and removed the classic editor. (Users could and, for now, still can
use the classic editor by installing a plugin.) The idea was to move to a system of <a href="https://developer.wordpress.org/block-editor/reference-guides/block-api/">blocks</a>
that make up the content and layout of a WordPress post or page. Each
element is a separate block: paragraphs, headings, images, and so forth
are each discrete blocks with properties that can be edited
separately. The screenshot below shows the Gutenberg editor in
WordPress 6.6.2 (the most recent as of this writing), with a paragraph
block selected. It has its own, contextual, menu of options for
formatting, alignment, or converting the block to another type.</p>

<blockquote>
<a href="/Articles/992836/#gutenberg">
<img alt="[Gutenberg Editor]" class="photo" hspace="5" src="https://static.lwn.net/images/2024/gutenberg-editor-sm.png" title="Gutenberg Editor" vspace="5"></img></a>
</blockquote>

<p>That represented
a major departure from the way users and developers were used to
managing content in WordPress. Since the initial Gutenberg release,
WordPress has moved to a <a href="https://www.smashingmagazine.com/2022/10/wordpress-full-site-editing/">full-site
editing</a> (FSE) approach that expands the blocks editing concept to
editing all aspects of a site (not just the page or post content)
using blocks and a GUI.</p>

<p>As with any major change in a popular open-source project, Gutenberg
caused a fair amount of <a href="https://watermelonwebworks.com/gutenberg-controversy/">controversy</a>.
Some of that could be fairly summed up as "people hate change". The
Gutenberg approach to editing content is something of an acquired taste,
and many users simply did not see value in a block-based editor for
writing blog posts. It also meant that some themes and plugins would
need to be updated to adapt to the block editor and its underlying
changes. But not all of the objections were a matter of taste:
Gutenberg's development had a real negative impact on accessibility in
WordPress.</p>

<p>Adrian Roselli <a href="https://adrianroselli.com/2018/12/lessons-from-gutenberg.html">wrote</a>
about those problems in 2018. Roselli identified several
problems, such as failure to staff the project with accessibility
experts, an accelerated schedule, and unfamiliar technology (Gutenberg
used the React framework, which was new to WordPress) led to problems
addressing accessibility issues caused by the move to Gutenberg. The
team that did work on WordPress accessibility, he said, were
volunteers facing "<q>a constantly shifting codebase</q>" where issues
fixed one day might reappear a week later. Rian Rietveld, who had been the WordPress
accessibility team lead, <a href="https://rianrietveld.com/2018/10/i-have-resigned-the-wordpress-accessibility-team/">resigned</a>
due to problems with the development process for Gutenberg and the lack
of emphasis on accessibility.</p>

<p>Accessibility aside, many other WordPress users simply have not
acquired the taste for the new editor (it has more than 2,400 one-star ratings on <a href="https://wordpress.org/support/plugin/gutenberg/reviews/">its
review page</a>, out of fewer than 4,000 reviews) or its implementation.
David Bushell <a href="https://dbushell.com/2021/08/03/wordpress-has-a-gutenberg-problem/">wrote</a>
in 2021 that Gutenberg "<q>has been a constant thorn in my side</q>"
with "<q>so many problem to discuss it's overwhelming</q>" and claimed
that the majority of web developers he knows disable the editor. Users
can, for now, still enable the <a href="https://wordpress.org/plugins/classic-editor/">classic editor via
a plugin</a> which restores the old editor and hides the block editor.
It is an official plugin that was <a href="https://make.wordpress.org/core/2018/11/07/classic-editor-plugin-support-window/">due
to sunset in 2021</a>, and now "<q>will be fully supported and
maintained until 2024, or as long as necessary</q>" according to the
plugin page. It has currently more than 10 million active installations,
so one hopes it is still considered necessary for some time to come.</p>

<h4>ClassicPress fork</h4>

<p>The fallout from Gutenberg led to a <a href="https://www.classicpress.net/classicpress-a-fork-of-wordpress-without-gutenberg/">fork</a> of WordPress called
ClassicPress in 2018. The
project <a href="https://www.classicpress.net/about/">provides</a> a
"<q>pre-5.0 WordPress publishing experience</q>" with <a href="https://www.classicpress.net/governance/">a governance process</a> and a <a href="https://www.classicpress.net/announcing-classicpress-nonprofit/">ClassicPress
Initiative non-profit</a> that covers the project's expenses.
It currently has <a href="https://opencollective.com/classicpress#category-BUDGET">an annual
budget</a> of less than $2,000. There is no for-profit entity behind the
project, and the foundation does not pay any developers to work on the
project. ClassicPress began with a fork of WordPress 4.9.x, the last
major version without Gutenberg, and the ClassicPress 1.x releases were
based on that branch. The most recent is <a href="https://forums.classicpress.net/t/classicpress-1-7-3-release-notes/">ClassicPress 1.7.3</a>, which was released in March 2024. Like WordPress, of
course, the project is licensed under the GPLv2 and is primarily
composed of PHP with a fair amount of JavaScript.</p>

<p>The ClassicPress project is not immune to controversy. In June 2022
the directors of the foundation <a href="https://forums.classicpress.net/t/next-steps-for-classicpress/4163">resigned</a>, with departing director Wade
Striebel saying that it was clear that "<q>the community feels that the
Directors of the ClassicPress Initiative are now hindering the
progress</q>" of the project. The outgoing board <a href="https://forums.classicpress.net/t/follow-up-on-next-steps/4188/4">appointed new
directors</a> who took over
the foundation, with a <a href="https://forums.classicpress.net/t/classicpress-initiative-and-project-plan/4195">plan for community governance</a>. The project <a href="https://forums.classicpress.net/t/the-future-of-classicpress-discussion/4480">decided</a>
via a vote by contributors in December 2022 to "<q>re-fork</q>"
from WordPress 6.x, rather than try
to be its own content-management system (CMS). It was felt that there
were not enough developers to keep the project relevant without
backporting code from upstream WordPress. There are currently nine
people identified as "<a href="https://forums.classicpress.net/g/Core_Committers">core
committers</a>", with 38 contributors who voted on the project direction.</p>

<p>The first release of ClassicPress based on WordPress 6.2.3 was <a href="https://www.classicpress.net/announcing-bella-version-2-of-classicpress/">announced</a>
in February 2024 as ClassicPress 2.0, and the most recent release in
that series (2.2) was <a href="https://www.classicpress.net/classicpress-version-2-2-is-out-2/">announced</a>
in September 2024. It pulled in some of the new WordPress features such
as the <a href="https://wordpress.org/documentation/article/site-health-screen/">Site
Health</a> status check, compatibility with more themes and plugins (so
long as they do not require blocks), as well as security updates.</p>

<p>While the primary selling point for ClassicPress is that it preserves
the pre-Gutenberg WordPress experience, the project has added some
enhancements unique to ClassicPress. For example, the 2.0 series
introduced HTML5 output by default, and accessibility improvements for using
widgets, menus, and other controls. The latest release made some small
enhancements to the media library to allow bulk editing of images or
other content stored in the library. For the most part, though, the
ClassicPress is more about retaining the original WordPress experience
and not about unique features.</p>

<h4>Moving to ClassicPress</h4>

<p>If starting from scratch, users can <a href="https://www.classicpress.net/get-classicpress/#install-classicpress">install
ClassicPress</a> more or less the same way that they would install
WordPress. It requires a server with PHP, MySQL or MariaDB, and a web
server (Apache, NGINX, or LightSpeed/OpenLightSpeed). The user needs
to be able to upload files to the server, and know the username,
password, and other database-access details to add the credentials to the
<tt>wp-config.php</tt> file. The rest is simply connecting to the web-based
administrative interface and walking through a brief setup routine. It is slightly
more involved than that, but only slightly.</p>

<p>It is even easier for users with an existing WordPress site. They can switch to ClassicPress
by installing and using the <a href="https://github.com/ClassicPress/ClassicPress-Migration-Plugin/releases">migration
plugin</a>. After the plugin is enabled, it runs a check against installed
themes and plugins to verify whether they are compatible with
ClassicPress or not, and recommends disabling those plugins for the migration.
Users are advised to install and use the WordPress <a href="https://wordpress.org/themes/twentyseventeen/">Twenty Seventeen
theme</a> to ensure compatibility during the migration. Users can
re-enable compatible plugins and themes after performing the migration,
if they do not depend on blocks. The actual migration takes just a few
seconds. It is an easy process, so long as the site does not depend on
any plugins or themes that use blocks.</p>

<p>Once installed (or migrated), ClassicPress looks and feels like
old-school WordPress. It has the familiar menu structure, administrative
interface, and (of course) the standard rich-text editor that so many
users still prefer. Fans of the <a href="https://make.wordpress.org/cli/handbook/">WP-CLI tool</a> will be
happy to know that it works well with ClassicPress too.</p>

<blockquote>
<a href="/Articles/992836/#classic">
<img alt="[Classic Editor]" class="photo" hspace="5" src="https://static.lwn.net/images/2024/classic-editor-sm.png" title="Classic Editor" vspace="5"></img></a>
</blockquote>

<p>ClassicPress is also a little bit more responsive than standard
WordPress, and much slimmer. The source for a WordPress install
is 76MB uncompressed, while ClassicPress weighs in at 37MB.</p>

<h4>Plugins and such</h4>

<p>The ClassicPress dashboard allows installation of themes and
plugins via the directory on WordPress.org. The dashboard displays all of
the available themes and plugins, but shows a warning and blocks
installation of those known to be incompatible with ClassicPress. For
example, if a theme requires FSE capabilities, it cannot be installed
through the dashboard.</p>

<p>Note that a plugin or theme may still have an
incompatibility that slips through the cracks. For example, I installed the WP fail2ban plugin on ClassicPress 2.2 and
WordPress 6.6.2 sites running on the same host. The plugin gave an error
about permissions when trying to access its Settings tab on
ClassicPress, but worked fine on WordPress. The other plugins and themes
I've tried, such as <a href="https://wordpress.org/plugins/antispam-bee/">Antispam Bee</a> for
fighting comment spam, have worked without any problems. (Or at least
any problems that have come to my attention.)</p>

<p>The project also has lists of <a href="https://directory.classicpress.net/plugins/">plugins</a> and <a href="https://directory.classicpress.net/themes/">themes</a> created
expressly for ClassicPress, as well as an <a href="https://directory.classicpress.net/plugins/classicpress-directory-integration/">integration
plugin</a> that allows users to manage them from the dashboard as they
would WordPress themes and plugins. There are fewer than 100 plugins and 13 themes custom-made for ClassicPress at the moment. Whether this is a
problem or not, of course, depends on what a user wants to do with their
CMS. For WordPress's original purpose, blogging, ClassicPress is just
fine.</p>

<p>For a business web site with all the bells and whistles, it may
depend. ClassicPress does have a project based on a fork of the <a href="https://woocommerce.com/">WooCommerce</a> online-store platform
for WordPress, called <a href="https://classiccommerce.cc/">Classic
Commerce</a>, for those who want a "<q>business-focused CMS</q>" with
e-commerce functions. It is missing an answer to <a href="https://jetpack.com/">Jetpack</a>, which is Automattic's bundle of
free and paid tools for things like malware scanning, backups,
statistics, and more. It is possible to <a href="https://www.diversetechgeek.com/recommended-jetpack-plugin-alternatives-wordpress/">cobble
together</a> much of that functionality through other plugins.</p>

<p>One of the primary concerns many users will have is about security
updates for their CMS. WordPress has been known to have a security issue
or two, and one might wonder whether the ClassicPress team is on top of
security updates that might affect it. Tim Kaye, one of the core
developers, told me that the project has backported "<q>all relevant
security updates</q>" from WordPress, though he also asserted that most
of the security updates "<q>don't apply to us because they are
blocks-related</q>". He recommended users refer to the <a href="https://github.com/ClassicPress/ClassicPress/pulls?q=is%3Apr+is%3Aclosed">merged
pull requests</a> to audit the security updates applied to
ClassicPress.</p>

<h4>Why bother?</h4>

<p>The ClassicPress effort may strike some as quixotic. Currently,
users can still grab the Classic Editor plugin and pretend Gutenberg
doesn't exist. However, there's no promise that it will be supported
forever. The block-editor experience is, for many, an unwanted
departure from the way they have worked with their site for
years. That workflow seems worth preserving, even if it is for a minority
of users.</p>

<p>The
project encourages <a href="https://github.com/ClassicPress/ClassicPress/blob/develop/.github/CONTRIBUTING.md">contribution</a>,
and has an <a href="https://forums.classicpress.net/">active forum</a>
and <a href="https://classicpress.zulipchat.com/register/">real-time
discussions via Zulip</a> (sign-up required). ClassicPress seems to be
a small, but healthy, project. For users who are disillusioned with
the direction of WordPress, or simply want to avoid the newfangled
editing system, it provides a useful alternative without having to
port content to a completely new CMS.</p>

<p></p>
<p><a href="/Articles/992219/#Comments">Comments (5 posted)</a>
</p><p>
</p>
                
                            </section>
        
    </div>
 </body>
</html>
